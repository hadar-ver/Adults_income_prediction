# Binary prediction for annual income of adults in the USA
this project was done together with **Roni Nakash** as part of **Machine learning and Data mining** course at BGU.

In this project, we will use several different supervised algorithms to precisely predict individuals’ income using Adult data Set collected from the UCI machine learning repository .  The model which we choose to use are model that can be handled with supervised learning and binary classification: logistic regression, random forest, SVC, KNN and decision tree (DT will be used especially to compare the RF results to DT results). 
We will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. 

# Business understanding:
Any  data  mining  project  starts  with  the  project's  goal  definition  that  is  included  in  the  first  phase “Business Understanding” [1].This initial phase focuses on understanding the project objectives and requirements from a business perspective, and then converting this knowledge into a data mining problem definition, and a preliminary project plan designed to achieve the objectives [2].
Our goal is to build a model that accurately predicts whether an individual makes more than $50,000 a year or not, this data can be useful for different purposes, for example: For Targeted advertising. Targeted advertising is a form of advertising, including online advertising, that is directed towards an audience with certain traits, based on the product or person the advertiser is promoting. These traits can either be demographic with a focus on race, economic status, sex, age, generation, level of education, income level, and employment, or there can be a psychographic focus which is based on the consumer values, personality, attitude, opinion, lifestyle and interest.  Targeted advertising is focused on certain traits and consumers who are likely to have a strong preference. These individuals will receive messages instead of those who have no interest and whose preferences do not match a particular product's attributes. 
Traditional forms of advertising, including billboards, newspapers, magazines, and radio channels, are progressively becoming replaced by online advertisements. The Information and communication technology (ICT) space has transformed recently, resulting in targeted advertising stretching across all ICT technologies, such as web, IPTV, and mobile environments. In the next generation's advertising, the importance of targeted advertisements will radically increase, as it spreads across numerous ICT channels cohesively. 
Through the emergence of new online channels, the need for targeted advertising is increasing because companies aim to minimize wasted advertising by means of information technology. Most targeted new media advertising currently uses second-order proxies for targets, such as tracking online or mobile web activities of consumers, associating historical web page consumer demographics with new consumer web page access, using a search word as the basis of implied interest, or contextual advertising. 

# Data understanding:
During  the  “Data  Understanding” phase  hypotheses  for hidden information regarding the data mining project goal are formed based on experience and qualified assumptions [1]. The data understanding phase starts with an initial data collection and proceeds with activities in order to get familiar with the data, to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information. There is a close link between Business Understanding and Data Understanding. The formulation of the data mining problem and the project plan require at least some understanding of the available data [2].
For real data, we used the Adult dataset, from the UCI Machine Learning Repository, which was drawn from the 1994 United States Census Bureau data and involves using personal details such as education level to predict whether an individual will earn more or less than $50,000 per year. The Adult dataset contains 32561 rows and 15 columns that are a mixture of categorical, ordinal, and numerical data types (as described in the table below). The null values presented with ‘?’ so we convert them to null to better understand our data.  The complete list of features and the statistical parameters for each is below:

![image](https://user-images.githubusercontent.com/71387302/198254511-14c8d435-ecaa-470a-8457-866e737eb4ea.png)

### Distribution plots for numerical features:
As we can see, not all the features are normally distributed. Some of the ML algorithms that we will use to predict has the assumption that the data is normally distributed- therefor, in the next phase- ‘Data preparation’, we will try to manipulate the data in such a way that it will be normally distributed. 
![image](https://user-images.githubusercontent.com/71387302/198254569-04c0a206-8647-4963-b127-9dc637f93bd6.png)

### ‘Skew table’-
Also presenting how much close to normal the variables are (good range would be: -0.5 up until 0.5) if the dataset skewed, then the ML model wouldn’t be able to do a good job of prediction:
![image](https://user-images.githubusercontent.com/71387302/198254666-4a5721b6-39e8-4dff-b9f5-ebd14c0f7ed5.png)

### Boxplot- 
Another way to look at the numerical data is to see the boxplot for each column divided by target values: in this way we can already see that we have different statistic parameters and distribution for different target values in these features: age, ‘education.num’ and weekly working hours- It may imply us that these features will have strong effect on our ML models (appendix 1). 
### Categorial features plots-
We also have looked on the categorial features plots and based on it we have decided to taka actions on some of them during the ‘Data preparation’ phase that will be described in the next Chapter.
###Correlations between features-
To have 1st intuition about our valuables we have checked the correlations of these features with each other (high correlation make one of them unnecessary)  and with target variable (high correlation may imply on possible strong feature). 
**For numerical features**:we’ve converted the target variable into binary (0 for income <=$50K , 1 for income > $50K).
**For nominal features**: To analyze correlations between categorial columns and target column we will use Chi-square test: In appendix 2 there is an example of code for the ‘Race’ column. For ‘Race’, like all other columns we have rejected H0- which means that there is a relationship between each of these columns to the target column. 

This phase gave intuitions about our data and based on these intuitions and on our conclusions- we have moved forward to the next phase- ‘Data preparation’.

# Data preparation:
In the “Data Preparation” phase the engineer collects the relevant data and prepares it for the actual data mining task.  This includes the preprocessing, e.g.  data reduction and filtering, as well as feature generation with respect to the data mining project goal [1] .The data preparation phase covers all activities to construct the final dataset (data that will be fed into the modeling tool(s)) from the initial raw data. Data preparation tasks are likely to be performed multiple times, and not in any prescribed order [2].
The dataset we used for this assignment is very messy and there are many preparations need to be done.
These are the preparations we performed on the data:
1.	**Outliers**: we have decided not to remove any outliers even though we found some because all outliers make sense. For example- very high ‘capital gain’. We also found outliers in the ‘hours.per.week’ feature but we are taking into consider that some of the people might work as housekeepers or nannies and their work is around the clock. 
2.	**Null values**: in this data set there are 3 features with null values: ‘WorkClass’- with 5.6% null values, ‘Occupation’ with 5.6% null values and ‘native.country’ with 1.8% null values. Because the null values are between 1.8%-5.6% and because we assume that these null values are random, we will remove these samples. After removing these samples, we saw visually that the proportion of the different values in these variables remained the same. 
3.	**Transformation**:
  a.	Log transformation: we checked the skewness of the data and we find out the data is skewed (the range should be between -0.5 to 0.5): In skewed data, the tail region may act as an outlier for the statistical model and we know that outliers adversely affect the model’s performance especially regression-based models. There are statistical model that are robust to outlier (like a Tree-based models) but it will limit the possibility to try other models. So, there is a necessity to transform the skewed data to close enough to a Gaussian distribution which will allow us to try a greater number of statistical models.
  Therefor we will use log for this mission, and we will transform age and ‘fnlwgt’. Capital (gain & loss) will be converted afterwards to a new feature so we will not perform transformation on these features.
  The results of this transformation led to distribution which are closer to Gaussian distribution:
![image](https://user-images.githubusercontent.com/71387302/198255305-029b2631-af92-4fd0-b22d-80d7c2583676.png)
  b.	Discretization: binning ‘age’ and ‘Hours.per.week’ according to Quantile-based discretization function. The function defines the bins using percentiles based on the distribution of the data, not the actual numeric edges of the bins
4.	Feature Representation:
  a.	**‘Marital status’**:  contains 7 categories, we decided to combine then into 2: ‘Single’ or ‘Married’
  b.	**‘Race’**: unified into 3 categories instead of 5: 86% are white, 9% black and 5% are other races. We combined all other races into 1 category called ‘Other’
  c.	**‘Native country’**: unified into 2 categories instead of 41: 90% of the population in this data set are from USA, the rest 10% are from 40 other countries. Because we don’t have enough representation for these countries, we unified them all into 1 category ‘Other’.
  d.	**‘Education’**: we combined the lower grades of education together into 1 category: ‘School’.
  e.	**‘Occupation’**: we combined 3 occupation with the lowest representation into 1 category: ‘Other’.
  f.	**New Feature- ‘Capital Diff’**: calculated feature of: ‘Capital gain’ – ‘Capital loss’
  g.	**‘Capital Diff’**: unified into 3 categories: 0- for capital loss, 1 for no loss and no gain, 2 for capital gain.
5.	Encoding: some of the Machine learning models, require all input and output variables to be numeric. We use encoding methods for this purpose. There are many encoding techniques, we decided to use 2 of them:
  a.	**Label encoder-** we used this method to encode ordinal feature by converting each value in a column to a number.
  b.	**One-hot encoder-** for non-ordinal features we didn’t use the label encoder technique because The problem using the number is that they introduce relation/comparison between them, so the algorithm might misunderstand that data and might give higher weight for higher numbers. Therefore, for non-ordinal features we used ‘One-hot’ encoder which the original variable is removed, and one new binary variable is added for each unique value in the variable.
6.	Feature reduction- after the encoding we had 52 features. To analyze which features will might give us better value for the models, we plot correlation matrix for all features (all features are numerical or binary- so we can use this matrix for all the features) and then drop features with low correlation with target variable (<5%)  and also features with high correlation with each other (we drop one of the correlated features- the one that has lower correlation with target). For example: ‘Sex_female’ and ‘Sex_male’ are 100% correlated, ‘edication_school’ & ‘education.num’ are 66% correlated so one of features in each pair is not necessary and need to be dropped. 
Now, our data set is clean, contains 31 features and ready to use.

# Modeling:
In  the  “Modeling” phase  a  data  mining  workflow  is  constructed to find the desired parameter settings for the selected algorithms and to execute the data mining task on the preprocessed data. [1]. Typically, there are several techniques for the same data mining problem type. Some techniques require specific data formats. There is a close link between Data Preparation and Modeling. Often, one realizes data problems while modeling or one gets ideas for constructing new data [2]. The goal of our model is to predict if individual’s annual income is above/below $50K. We will be implemented ‘StratifiedKFold’ for splitting the data into train & test sets:  This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.
We used few models to predict our binary target: ‘Income’. Models results will be discussed on ‘evaluation’ stage.
For each model we used ‘GridSearch’ function to perform hyper parameter tuning, which evaluate combination of parameters and choose the one that provide the best results according to selected parameters range and metric.
### Logistic regression
Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval, or ratio-level independent variables. Sometimes logistic regressions are difficult to interpret.
The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation. The sigmoid classifier that will help us make this decision. Consider a single input observation x, which we will represent by a vector of features [x1, x2,..., xn]. The classifier output y can be 1 (meaning the observation income is above $50K) or 0 (equal or below $50K). We want to know the probability P(y = 1|x) that this observation is a member of the class. Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight is a real number and is associated with one of the input features. The weight represents how important that input feature is to the classification decision and can be positive or negative (providing evidence that the instance being classified belongs in the negative class). Logistic regression AUC on our data set: 89.4% (will be discussed in ‘evaluation’ stage)
### Decision tree
A DT represents a set of restrictions or conditions which are hierarchically organized, and which are successively applied from a root to a terminal node or leaf of the tree. The main benefit of using a hierarchical tree structure to perform classification decisions is that the tree structure is transparent, which makes it is easier to interpret. To induce the DT from a dataset, an evaluation measure of each of the evidential features is used to maximize the inter node heterogeneity. From the root node, the data splitting process in each internal node of a rule of the tree is repeated until a stop condition previously specified is reached. Each of the terminal nodes, or leaves, has attached to it a simple regression model which applies in that node only. Once the tree's induction process is finished, pruning can be applied with the aim of improving the tree's generalization capacity by reducing its structural complexity. The number of cases in nodes can be taken as pruning criteria. The induction of the DT involves first selecting optimal splitting measurement vectors. The process starts by splitting the dependent feature, or the parent node (root), into binary pieces, where the child nodes are ‘purer’ than the parent node. Through this process, the DTs search through all candidate splits to find the optimal split, s*, that maximizes the ‘purity’ of the resulting tree (as defined by the largest decrease in the impurity). [3]  DT AUC on our data set: 74.4% with default parameters on test set, while Train set was almost perfect. This is a situation of overfit because our tree was not limited so he fir himself almost perfectly to the data he observed (train data). After hyper parameter tuning the result was much better: 90.4% (+16%). the hyper parameter tuning has also impact on the train results which are now almost like the test results and therefore we got better generalization capabilities.
### Random Forest
RF is a regression technique that combines the performance of numerous DT algorithms to classify or predict the value of a variable. That is when RF receives an (x) input vector, made up of the values of the different evidential features analyzed for a given training area, RF builds a number K of regression trees and averages the results. To avoid the correlation of the different trees, RF increases the diversity of the trees by making them grow from different training data subsets created through a procedure called bagging. Hence, some data may be used more than once in the training, while others might never be used. Thus, greater stability is achieved, as it makes it more robust when facing slight variations in input data and, at the same time, it increases prediction accuracy. On the other hand, when the RF makes a tree grow, it uses the best feature/split point within a subset of evidential features which has been selected randomly from the overall set of input evidential features. Therefore, this can decrease the strength of every single tree, but it reduces the correlation between the trees, which reduces the generalization error. Another characteristic of interest is that the trees of a RF classifier grow with no pruning, which makes them light, from a computational perspective. [3] Random forest AUC on our data set: 87.7%, and similar to DT results- the train data also perform almost perfect results (overfit). After hyper parameter tuning the results was much better: 90.6% (+2.9%) with high generalization capabilities.
### SVC
SVC is a supervised method to perform dichotomy classification of multidimensional feature-vectors. Originally, it was developed as a linear classification method, generalized later to a non-linear classifier and, later, it was extended to regression problems. The basic idea under the SVM method is to transform the input features into a higher-dimensional space where the two classes can be linearly separated by a high-dimensional surface, known as hyper-plane [3]. Algorithmically, support vector machines build optimal separating boundaries between data sets by solving a constrained quadratic optimization problem . By using different kernel functions, varying degrees of nonlinearity and flexibility can be included in the model. The disadvantage of support vector machines is that the classification result is purely dichotomous, and no probability of class membership is given. [4] SVC AUC on our data set: 89.5% . After hyper parameter tuning the results was much better: 90.3% (+0.8%)
### KNN
Classification based on the k-nearest neighbor algorithm differs from the other methods considered here, as this algorithm uses the data directly for classification, without building a model first . As such, no details of model construction need to be considered, and the only adjustable parameter in the model is k, the number of nearest neighbors to include in the estimate of class membership: the value of P(y|x) is calculated simply as the ratio of members of class y among the k nearest neighbors of x. By varying k, the model can be made flexible (small or large values of k, respectively). The advantage that k-nearest neighbors have over other algorithms is the fact that the neighbors can provide an explanation for the classification result; this  can provide an advantage in areas where black-box models are inadequate. [4] KNN AUC on our data set: 82.4%. After hyper parameter tuning the results was much better: 86.9% (+4.5%)

# Evaluation & improvements
Within the subsequent “Evaluation” phase the trained model  is  tested  against  real  data  sets  within  a  production  scenario  and  the  data  mining  results  are  assessed  according  to  the  underlying  business  objectives.  For this purpose, test  data  sets  are  generated  following  the  steps  developed in  the  “Data  Preparation”  and  “Modeling”  phases excluding the labeling step. After successful evaluation of the trained model, it is  deployed  into  production  in  the  “Deployment” phase. However, the deployment also requires a stable set-up for data acquisition  including  a  sensor  and  data  processing infrastructure [1]. We will evaluate the model’s performance by calculate the Area Under the Curve (AUC) of the  Receiver Operating Characteristic (ROC), which is a performance metric for binary classification problems. The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. Below, you can find the ROC curve for all models, after hyper parameter tuning. For each one of them separately you can find in Appendix 3. ROC curve for all models together presented in the right figure:
![image](https://user-images.githubusercontent.com/71387302/198256059-73c5fdee-997c-4364-90f0-68900a5365e7.png)

### Improvements for logistic regression:
The ROC for logistic regression was 89.4% (on test set). To try to improve this model results we normalized the numerical features and added polynomial features to the model pipeline- the polynomial features generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to 2.  We saw that the model results went from 89.4% to 90.9% (+1.5%)
![image](https://user-images.githubusercontent.com/71387302/198256093-d7d75439-f614-4bff-866a-27f8a490d36a.png)

Since our data set is not balanced we also added to the ROC metric the Macro & Weighted avg accuracy metrics, so we will be able to choose the best model.
**•	Macro F1 avg accuracy ** calculates the F1 separated by class: F1class1+F1class2+⋅⋅⋅+F1classN
**•	Weighted F1 avg accuracy** calculates the F1 score for each class independently but when it adds them together uses a weight that depends on the number of true labels of each class:   F1class1∗W1+F1class2∗W2+⋅⋅⋅+F1classN∗WN
 Below is the summary table for all models with different evaluation metrics:

![image](https://user-images.githubusercontent.com/71387302/198256295-7fc9ed0b-f13a-46bb-85e8-41fbc27de767.png)

### Possible extension:
If we want to extend our work, we will consider:
1.	Try some improvement on other models although we are satisfying with the results we got.
2.	Examine other feature reduction methods and test the model results according  to teach one of these methods.



